{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f6uIA8uC2PQ_"},"outputs":[],"source":["from collections import defaultdict\n","import numpy as np\n","from collections import defaultdict\n","import random\n","def evaluate_precision(df_recommendations, df_interactions, k=10, max_users=1000):\n","    user_books = defaultdict(set)\n","    for _, row in df_interactions.iterrows():\n","        user_books[row['user_id']].add(row['book_id'])\n","\n","    total_hits = 0\n","    total_preds = 0\n","    total_truth = 0\n","\n","    for book_id in df_recommendations['input_book_id'].unique():\n","        recommended_books = df_recommendations[\n","            df_recommendations['input_book_id'] == book_id\n","        ]['recommended_book_id'].tolist()[:k]\n","\n","        users = [user for user, books in user_books.items() if book_id in books]\n","        if len(users) > max_users:\n","            users = random.sample(users, max_users)\n","\n","        for user in users:\n","            books = user_books[user] - {book_id}\n","            hits = sum([1 for r in recommended_books if r in books])\n","            total_hits += hits\n","            total_preds += k\n","\n","    precision = total_hits / total_preds if total_preds > 0 else 0\n","    return precision\n","\n","def evaluate_triplet_loss(sim_df, interaction_df, margin=0.1, max_users=1000):\n","    user_groups = interaction_df[interaction_df[\"is_read\"] == True].groupby(\"user_id\")\n","\n","    all_users = list(user_groups.groups.keys())\n","    if len(all_users) > max_users:\n","        sampled_users = set(random.sample(all_users, max_users))\n","    else:\n","        sampled_users = set(all_users)\n","\n","    triplet_losses = []\n","\n","    sim_dict = sim_df.groupby(\"input_book_id\").apply(\n","        lambda df: dict(zip(df[\"recommended_book_id\"], df[\"similarity\"]))\n","    ).to_dict()\n","    for user, group in user_groups:\n","        if user not in sampled_users:\n","            continue\n","        read_books = list(set(group[\"book_id\"]))\n","        if len(read_books) < 2:\n","            continue\n","        for anchor in read_books:\n","            positives = [b for b in read_books if b != anchor]\n","            all_candidates = sim_dict.get(anchor, {})\n","            for pos in positives:\n","                sim_ap = all_candidates.get(pos)\n","                if sim_ap is None:\n","                    continue\n","                for neg, sim_an in all_candidates.items():\n","                    if neg in read_books:\n","                        continue\n","                    loss = max(0.0, sim_an - sim_ap + margin)\n","                    triplet_losses.append(loss)\n","\n","    return sum(triplet_losses) / len(triplet_losses) if triplet_losses else None\n","\n","\n","\n","def improvement_score(metric_sbert, metric_collab, higher_is_better=True, eps=1e-8):\n","    if higher_is_better:\n","        return (metric_sbert - metric_collab) / (abs(metric_collab) + eps)\n","    else:\n","        return (metric_collab - metric_sbert) / (abs(metric_collab) + eps)\n","\n","def evaluate_all_models(sbert_df, collab_df, interaction_df, k=10):\n","    # Precision\n","    precision_sbert = evaluate_precision(sbert_df, interaction_df, k)\n","    precision_collab = evaluate_precision(collab_df, interaction_df, k)\n","\n","    # Triplet Loss\n","    triplet_sbert = evaluate_triplet_loss(sbert_df, interaction_df)\n","    triplet_collab = evaluate_triplet_loss(collab_df, interaction_df)\n","\n","    result = {\n","        \"Precision@{}\".format(k): {\n","            \"sbert\": precision_sbert,\n","            \"collab\": precision_collab,\n","            \"improvement\": improvement_score(precision_sbert, precision_collab, True)\n","        },\n","        \"TripletLoss\": {\n","            \"sbert\": triplet_sbert,\n","            \"collab\": triplet_collab,\n","            \"improvement\": improvement_score(triplet_sbert, triplet_collab, False)\n","        }\n","    }\n","\n","    return result\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e2WmQDtJynF","executionInfo":{"status":"ok","timestamp":1748020722869,"user_tz":420,"elapsed":14534,"user":{"displayName":"JINRUI WEN","userId":"01372039510104793799"}},"outputId":"89159bf8-7979-4613-ab9b-deec08aa988e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from collections import defaultdict\n","import random\n","import os\n","import gzip\n","import json\n","\n","df_collab = pd.read_csv('/content/drive/MyDrive/Capstone/dataset/recommendercollab.csv')\n","df_sbert = pd.read_csv('/content/drive/MyDrive/Capstone/output_batches/recommender_all_dedup.csv')\n","df_interactions = pd.read_csv('/content/drive/MyDrive/Capstone/dataset/interaction_part.csv')\n","df_interactions.columns = df_interactions.columns.str.lower()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNMXOiiBIR2W","executionInfo":{"status":"ok","timestamp":1748020852130,"user_tz":420,"elapsed":14553,"user":{"displayName":"JINRUI WEN","userId":"01372039510104793799"}},"outputId":"0880a8c0-3b7c-43ae-eef0-71d535540f77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-f64530e686a1>:10: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_interactions = pd.read_csv('/content/drive/MyDrive/Capstone/dataset/interaction_part.csv')\n"]}]},{"cell_type":"code","source":["results = evaluate_all_models(df_sbert, df_collab, df_interactions, k=10)\n","\n","print(\"=== Model Comparison ===\")\n","for metric, values in results.items():\n","    print(f\"{metric}:\")\n","    print(f\"  SBERT   : {values['sbert']:.4f}\")\n","    print(f\"  Collab  : {values['collab']:.4f}\")\n","    print(f\"  Improvement: {values['improvement']:.2%}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvQh75ysKTQY","executionInfo":{"status":"ok","timestamp":1748035344611,"user_tz":420,"elapsed":14110683,"user":{"displayName":"JINRUI WEN","userId":"01372039510104793799"}},"outputId":"fe57a942-c058-4099-f763-c7a0390d7b6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-b8ace87888a3>:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  sim_dict = sim_df.groupby(\"input_book_id\").apply(\n","<ipython-input-8-b8ace87888a3>:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  sim_dict = sim_df.groupby(\"input_book_id\").apply(\n"]},{"output_type":"stream","name":"stdout","text":["=== Model Comparison ===\n","Precision@10:\n","  SBERT   : 0.2127\n","  Collab  : 0.4056\n","  Improvement: -47.55%\n","TripletLoss:\n","  SBERT   : 0.0953\n","  Collab  : 0.0765\n","  Improvement: -24.61%\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}